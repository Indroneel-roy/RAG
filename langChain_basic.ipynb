{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "952bd074",
   "metadata": {},
   "source": [
    "### ============================\n",
    "### Core RAG Concepts\n",
    "###  ============================\n",
    "\n",
    "###  1. Indexing:\n",
    "###  A pipeline that ingests data from a source (e.g., PDFs, web pages, databases),\n",
    "###  processes it, and stores it in an index (usually a vector database).\n",
    "###  This step typically runs as a separate, offline process.\n",
    "\n",
    "###  2. Retrieval and Generation:\n",
    "###  The real-time RAG workflow that:\n",
    "###  - Takes the user query at runtime\n",
    "###  - Retrieves the most relevant data from the indexed source\n",
    "###  - Passes the retrieved context to the language model\n",
    "###  - Generates the final response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25c15d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee712fa",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c34a151f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b999815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-1.1.2-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting langchain-text-splitters\n",
      "  Using cached langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting bs4\n",
      "  Using cached bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting langchain-core<2.0.0,>=1.1.0 (from langchain)\n",
      "  Downloading langchain_core-1.1.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting langgraph<1.1.0,>=1.0.2 (from langchain)\n",
      "  Downloading langgraph-1.0.4-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from langchain) (2.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (1.33)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.1.0->langchain)\n",
      "  Downloading langsmith-0.4.56-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\indroneel\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (25.0)\n",
      "Collecting pyyaml<7.0.0,>=5.3.0 (from langchain-core<2.0.0,>=1.1.0->langchain)\n",
      "  Using cached pyyaml-6.0.3-cp313-cp313-win_amd64.whl.metadata (2.4 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<2.0.0,>=1.1.0->langchain)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (4.15.0)\n",
      "Collecting uuid-utils<1.0,>=0.12.0 (from langchain-core<2.0.0,>=1.1.0->langchain)\n",
      "  Downloading uuid_utils-0.12.0-cp39-abi3-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain) (3.0.0)\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Using cached langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Downloading langgraph_sdk-0.2.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting xxhash>=3.5.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Using cached xxhash-3.6.0-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Using cached ormsgpack-1.12.0-cp313-cp313-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting httpx>=0.25.2 (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson>=3.10.1 (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Downloading orjson-3.11.5-cp313-cp313-win_amd64.whl.metadata (42 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (0.24.0)\n",
      "Collecting anyio (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Downloading anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2025.10.5)\n",
      "Collecting httpcore==1.* (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.7)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
      "  Using cached langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting SQLAlchemy<3.0.0,>=1.4.0 (from langchain-community)\n",
      "  Using cached sqlalchemy-2.0.44-cp313-cp313-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Using cached aiohttp-3.13.2-cp313-cp313-win_amd64.whl.metadata (8.4 kB)\n",
      "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from langchain-community) (2.10.1)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Using cached httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting numpy>=2.1.0 (from langchain-community)\n",
      "  Downloading numpy-2.3.5-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached frozenlist-1.8.0-cp313-cp313-win_amd64.whl.metadata (21 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached multidict-6.7.0-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached propcache-0.4.1-cp313-cp313-win_amd64.whl.metadata (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached yarl-1.22.0-cp313-cp313-win_amd64.whl.metadata (77 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (2.5.0)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community)\n",
      "  Downloading greenlet-3.3.0-cp313-cp313-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.14.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>=1.6.1 (from beautifulsoup4->bs4)\n",
      "  Using cached soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Downloading langchain-1.1.2-py3-none-any.whl (102 kB)\n",
      "Downloading langchain_core-1.1.1-py3-none-any.whl (475 kB)\n",
      "Downloading langgraph-1.0.4-py3-none-any.whl (157 kB)\n",
      "Using cached langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
      "Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\n",
      "Downloading langgraph_sdk-0.2.14-py3-none-any.whl (66 kB)\n",
      "Downloading langsmith-0.4.56-py3-none-any.whl (411 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached pyyaml-6.0.3-cp313-cp313-win_amd64.whl (154 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading uuid_utils-0.12.0-cp39-abi3-win_amd64.whl (183 kB)\n",
      "Using cached langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
      "Using cached langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "Using cached aiohttp-3.13.2-cp313-cp313-win_amd64.whl (452 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Using cached langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached multidict-6.7.0-cp313-cp313-win_amd64.whl (45 kB)\n",
      "Using cached sqlalchemy-2.0.44-cp313-cp313-win_amd64.whl (2.1 MB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached yarl-1.22.0-cp313-cp313-win_amd64.whl (86 kB)\n",
      "Using cached bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached frozenlist-1.8.0-cp313-cp313-win_amd64.whl (43 kB)\n",
      "Downloading greenlet-3.3.0-cp313-cp313-win_amd64.whl (301 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading numpy-2.3.5-cp313-cp313-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/12.8 MB 6.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.9/12.8 MB 7.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.7/12.8 MB 7.6 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 6.3/12.8 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.9/12.8 MB 7.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.4/12.8 MB 7.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.7/12.8 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.8/12.8 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 7.0 MB/s  0:00:01\n",
      "Downloading orjson-3.11.5-cp313-cp313-win_amd64.whl (133 kB)\n",
      "Using cached ormsgpack-1.12.0-cp313-cp313-win_amd64.whl (112 kB)\n",
      "Using cached propcache-0.4.1-cp313-cp313-win_amd64.whl (40 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached xxhash-3.6.0-cp313-cp313-win_amd64.whl (31 kB)\n",
      "Downloading anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "Downloading beautifulsoup4-4.14.3-py3-none-any.whl (107 kB)\n",
      "Using cached soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: xxhash, uuid-utils, tenacity, soupsieve, pyyaml, propcache, ormsgpack, orjson, numpy, mypy-extensions, multidict, marshmallow, httpx-sse, h11, greenlet, frozenlist, attrs, anyio, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests-toolbelt, httpcore, beautifulsoup4, aiosignal, httpx, dataclasses-json, bs4, aiohttp, langsmith, langgraph-sdk, langchain-core, langgraph-checkpoint, langchain-text-splitters, langgraph-prebuilt, langchain-classic, langgraph, langchain-community, langchain\n",
      "\n",
      "   ---- -----------------------------------  4/40 [pyyaml]\n",
      "   ------ ---------------------------------  6/40 [ormsgpack]\n",
      "   -------- -------------------------------  8/40 [numpy]\n",
      "   -------- -------------------------------  8/40 [numpy]\n",
      "   -------- -------------------------------  8/40 [numpy]\n",
      "   -------- -------------------------------  8/40 [numpy]\n",
      "   -------- -------------------------------  8/40 [numpy]\n",
      "   -------- -------------------------------  8/40 [numpy]\n",
      "   -------- -------------------------------  8/40 [numpy]\n",
      "   -------- -------------------------------  8/40 [numpy]\n",
      "   -------- -------------------------------  8/40 [numpy]\n",
      "   -------- -------------------------------  8/40 [numpy]\n",
      "   -------- -------------------------------  8/40 [numpy]\n",
      "   -------- -------------------------------  8/40 [numpy]\n",
      "   -------- -------------------------------  8/40 [numpy]\n",
      "   -------- -------------------------------  8/40 [numpy]\n",
      "   ------------ --------------------------- 12/40 [httpx-sse]\n",
      "   -------------- ------------------------- 14/40 [greenlet]\n",
      "   ----------------- ---------------------- 17/40 [anyio]\n",
      "   --------------------- ------------------ 21/40 [SQLAlchemy]\n",
      "   --------------------- ------------------ 21/40 [SQLAlchemy]\n",
      "   --------------------- ------------------ 21/40 [SQLAlchemy]\n",
      "   --------------------- ------------------ 21/40 [SQLAlchemy]\n",
      "   --------------------- ------------------ 21/40 [SQLAlchemy]\n",
      "   --------------------- ------------------ 21/40 [SQLAlchemy]\n",
      "   --------------------- ------------------ 21/40 [SQLAlchemy]\n",
      "   --------------------- ------------------ 21/40 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 23/40 [httpcore]\n",
      "   -------------------------- ------------- 26/40 [httpx]\n",
      "   ----------------------------- ---------- 29/40 [aiohttp]\n",
      "   ----------------------------- ---------- 29/40 [aiohttp]\n",
      "   ------------------------------ --------- 30/40 [langsmith]\n",
      "   ------------------------------- -------- 31/40 [langgraph-sdk]\n",
      "   -------------------------------- ------- 32/40 [langchain-core]\n",
      "   -------------------------------- ------- 32/40 [langchain-core]\n",
      "   --------------------------------- ------ 33/40 [langgraph-checkpoint]\n",
      "   ------------------------------------ --- 36/40 [langchain-classic]\n",
      "   ------------------------------------ --- 36/40 [langchain-classic]\n",
      "   ------------------------------------ --- 36/40 [langchain-classic]\n",
      "   ------------------------------------ --- 36/40 [langchain-classic]\n",
      "   ------------------------------------ --- 36/40 [langchain-classic]\n",
      "   ------------------------------------ --- 36/40 [langchain-classic]\n",
      "   ------------------------------------ --- 36/40 [langchain-classic]\n",
      "   ------------------------------------ --- 36/40 [langchain-classic]\n",
      "   ------------------------------------ --- 36/40 [langchain-classic]\n",
      "   ------------------------------------ --- 36/40 [langchain-classic]\n",
      "   ------------------------------------ --- 36/40 [langchain-classic]\n",
      "   ------------------------------------ --- 36/40 [langchain-classic]\n",
      "   ------------------------------------ --- 36/40 [langchain-classic]\n",
      "   ------------------------------------ --- 36/40 [langchain-classic]\n",
      "   ------------------------------------ --- 36/40 [langchain-classic]\n",
      "   ------------------------------------- -- 37/40 [langgraph]\n",
      "   -------------------------------------- - 38/40 [langchain-community]\n",
      "   -------------------------------------- - 38/40 [langchain-community]\n",
      "   -------------------------------------- - 38/40 [langchain-community]\n",
      "   -------------------------------------- - 38/40 [langchain-community]\n",
      "   -------------------------------------- - 38/40 [langchain-community]\n",
      "   -------------------------------------- - 38/40 [langchain-community]\n",
      "   -------------------------------------- - 38/40 [langchain-community]\n",
      "   -------------------------------------- - 38/40 [langchain-community]\n",
      "   -------------------------------------- - 38/40 [langchain-community]\n",
      "   -------------------------------------- - 38/40 [langchain-community]\n",
      "   -------------------------------------- - 38/40 [langchain-community]\n",
      "   -------------------------------------- - 38/40 [langchain-community]\n",
      "   -------------------------------------- - 38/40 [langchain-community]\n",
      "   -------------------------------------- - 38/40 [langchain-community]\n",
      "   -------------------------------------- - 38/40 [langchain-community]\n",
      "   -------------------------------------- - 38/40 [langchain-community]\n",
      "   -------------------------------------- - 38/40 [langchain-community]\n",
      "   -------------------------------------- - 38/40 [langchain-community]\n",
      "   -------------------------------------- - 38/40 [langchain-community]\n",
      "   ---------------------------------------  39/40 [langchain]\n",
      "   ---------------------------------------- 40/40 [langchain]\n",
      "\n",
      "Successfully installed SQLAlchemy-2.0.44 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 anyio-4.12.0 attrs-25.4.0 beautifulsoup4-4.14.3 bs4-0.0.2 dataclasses-json-0.6.7 frozenlist-1.8.0 greenlet-3.3.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.3 langchain-1.1.2 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-core-1.1.1 langchain-text-splitters-1.0.0 langgraph-1.0.4 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.2.14 langsmith-0.4.56 marshmallow-3.26.1 multidict-6.7.0 mypy-extensions-1.1.0 numpy-2.3.5 orjson-3.11.5 ormsgpack-1.12.0 propcache-0.4.1 pyyaml-6.0.3 requests-toolbelt-1.0.0 soupsieve-2.8 tenacity-9.1.2 typing-inspect-0.9.0 uuid-utils-0.12.0 xxhash-3.6.0 yarl-1.22.0\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain langchain-text-splitters langchain-community bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff0d3f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f76d2471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langsmith in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (0.4.56)\n",
      "Collecting langchain-groq\n",
      "  Downloading langchain_groq-1.1.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from langsmith) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from langsmith) (3.11.5)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\indroneel\\appdata\\roaming\\python\\python313\\site-packages (from langsmith) (25.0)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from langsmith) (2.12.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from langsmith) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from langsmith) (2.32.5)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from langsmith) (0.12.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from langsmith) (0.24.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from pydantic<3,>=1->langsmith) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from pydantic<3,>=1->langsmith) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from pydantic<3,>=1->langsmith) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from pydantic<3,>=1->langsmith) (0.4.2)\n",
      "Collecting groq<1.0.0,>=0.30.0 (from langchain-groq)\n",
      "  Downloading groq-0.37.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from langchain-groq) (1.1.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.9.0)\n",
      "Collecting sniffio (from groq<1.0.0,>=0.30.0->langchain-groq)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (1.33)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (9.1.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain-groq) (3.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from requests>=2.0.0->langsmith) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from requests>=2.0.0->langsmith) (2.5.0)\n",
      "Downloading langchain_groq-1.1.0-py3-none-any.whl (19 kB)\n",
      "Downloading groq-0.37.1-py3-none-any.whl (137 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: sniffio, groq, langchain-groq\n",
      "\n",
      "   ------------- -------------------------- 1/3 [groq]\n",
      "   ---------------------------------------- 3/3 [langchain-groq]\n",
      "\n",
      "Successfully installed groq-0.37.1 langchain-groq-1.1.0 sniffio-1.3.1\n"
     ]
    }
   ],
   "source": [
    "! pip install langsmith langchain-groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e639d",
   "metadata": {},
   "source": [
    "Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7f07a9",
   "metadata": {},
   "source": [
    "1. Select the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6d1ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"openai/gpt-oss-120b\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634c0b72",
   "metadata": {},
   "source": [
    "2. Select an embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc896a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb642943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Using cached torch-2.9.1-cp313-cp313-win_amd64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.7.2-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Using cached scipy-1.16.3-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Using cached pillow-12.0.0-cp313-cp313-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\indroneel\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached regex-2025.11.3-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.12.0)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.6-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\indroneel\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached markupsafe-3.0.3-cp313-cp313-win_amd64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\indroneel\\miniconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.5)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/12.0 MB 4.0 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.4/12.0 MB 4.5 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 4.2/12.0 MB 5.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.7/12.0 MB 6.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.6/12.0 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.4/12.0 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.0/12.0 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 7.1 MB/s  0:00:01\n",
      "Using cached regex-2025.11.3-cp313-cp313-win_amd64.whl (277 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Using cached torch-2.9.1-cp313-cp313-win_amd64.whl (110.9 MB)\n",
      "Downloading networkx-3.6-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 1.6/2.1 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 8.4 MB/s  0:00:00\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached markupsafe-3.0.3-cp313-cp313-win_amd64.whl (15 kB)\n",
      "Using cached pillow-12.0.0-cp313-cp313-win_amd64.whl (7.0 MB)\n",
      "Using cached scikit_learn-1.7.2-cp313-cp313-win_amd64.whl (8.7 MB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached scipy-1.16.3-cp313-cp313-win_amd64.whl (38.5 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: mpmath, threadpoolctl, sympy, scipy, safetensors, regex, Pillow, networkx, MarkupSafe, joblib, scikit-learn, jinja2, torch, transformers, sentence-transformers\n",
      "\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----------------------------------------  0/15 [mpmath]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   ----- ----------------------------------  2/15 [sympy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   -------- -------------------------------  3/15 [scipy]\n",
      "   ------------- --------------------------  5/15 [regex]\n",
      "   ---------------- -----------------------  6/15 [Pillow]\n",
      "   ---------------- -----------------------  6/15 [Pillow]\n",
      "   ------------------ ---------------------  7/15 [networkx]\n",
      "   ------------------ ---------------------  7/15 [networkx]\n",
      "   ------------------ ---------------------  7/15 [networkx]\n",
      "   ------------------ ---------------------  7/15 [networkx]\n",
      "   ------------------ ---------------------  7/15 [networkx]\n",
      "   ------------------ ---------------------  7/15 [networkx]\n",
      "   ------------------ ---------------------  7/15 [networkx]\n",
      "   ------------------ ---------------------  7/15 [networkx]\n",
      "   ------------------ ---------------------  7/15 [networkx]\n",
      "   ------------------ ---------------------  7/15 [networkx]\n",
      "   ------------------ ---------------------  7/15 [networkx]\n",
      "   ------------------ ---------------------  7/15 [networkx]\n",
      "   ------------------ ---------------------  7/15 [networkx]\n",
      "   ------------------ ---------------------  7/15 [networkx]\n",
      "   ------------------ ---------------------  7/15 [networkx]\n",
      "   ------------------ ---------------------  7/15 [networkx]\n",
      "   ------------------------ ---------------  9/15 [joblib]\n",
      "   ------------------------ ---------------  9/15 [joblib]\n",
      "   -------------------------- ------------- 10/15 [scikit-learn]\n",
      "   -------------------------- ------------- 10/15 [scikit-learn]\n",
      "   -------------------------- ------------- 10/15 [scikit-learn]\n",
      "   -------------------------- ------------- 10/15 [scikit-learn]\n",
      "   -------------------------- ------------- 10/15 [scikit-learn]\n",
      "   -------------------------- ------------- 10/15 [scikit-learn]\n",
      "   -------------------------- ------------- 10/15 [scikit-learn]\n",
      "   -------------------------- ------------- 10/15 [scikit-learn]\n",
      "   -------------------------- ------------- 10/15 [scikit-learn]\n",
      "   -------------------------- ------------- 10/15 [scikit-learn]\n",
      "   -------------------------- ------------- 10/15 [scikit-learn]\n",
      "   -------------------------- ------------- 10/15 [scikit-learn]\n",
      "   -------------------------- ------------- 10/15 [scikit-learn]\n",
      "   -------------------------- ------------- 10/15 [scikit-learn]\n",
      "   -------------------------- ------------- 10/15 [scikit-learn]\n",
      "   -------------------------- ------------- 10/15 [scikit-learn]\n",
      "   -------------------------- ------------- 10/15 [scikit-learn]\n",
      "   -------------------------- ------------- 10/15 [scikit-learn]\n",
      "   -------------------------- ------------- 10/15 [scikit-learn]\n",
      "   -------------------------- ------------- 10/15 [scikit-learn]\n",
      "   ----------------------------- ---------- 11/15 [jinja2]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   -------------------------------- ------- 12/15 [torch]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ---------------------------------- ----- 13/15 [transformers]\n",
      "   ------------------------------------- -- 14/15 [sentence-transformers]\n",
      "   ------------------------------------- -- 14/15 [sentence-transformers]\n",
      "   ------------------------------------- -- 14/15 [sentence-transformers]\n",
      "   ------------------------------------- -- 14/15 [sentence-transformers]\n",
      "   ---------------------------------------- 15/15 [sentence-transformers]\n",
      "\n",
      "Successfully installed MarkupSafe-3.0.3 Pillow-12.0.0 jinja2-3.1.6 joblib-1.5.2 mpmath-1.3.0 networkx-3.6 regex-2025.11.3 safetensors-0.7.0 scikit-learn-1.7.2 scipy-1.16.3 sentence-transformers-5.1.2 sympy-1.14.0 threadpoolctl-3.6.0 torch-2.9.1 transformers-4.57.3\n"
     ]
    }
   ],
   "source": [
    "! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffd11bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\indroneel\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\indroneel\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\indroneel\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22600362",
   "metadata": {},
   "source": [
    "3. Select a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "485d5a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "044f589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca23ab",
   "metadata": {},
   "source": [
    "### ============================\n",
    "### Indexing Pipeline (RAG)\n",
    "### ============================\n",
    "\n",
    "### Load:\n",
    "### Load raw data from different sources (PDFs, web pages, text files, databases)\n",
    "### using Document Loaders.\n",
    "\n",
    "### Split:\n",
    "### Break large documents into smaller text chunks using text splitters.\n",
    "### This helps with better search and fits content into the models context window.\n",
    "\n",
    "### Store:\n",
    "### Store and index the text chunks using a VectorStore and an Embeddings model\n",
    "### so they can be efficiently searched later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e66bc51",
   "metadata": {},
   "source": [
    "\n",
    "Loading documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f63711e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43047\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "196bcc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bc84ef",
   "metadata": {},
   "source": [
    "Splitting documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ec0f14",
   "metadata": {},
   "source": [
    "## Handling Long Documents\n",
    "\n",
    "Our loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n",
    "\n",
    "To handle this we'll split the `Document` into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\n",
    "\n",
    "we use a `RecursiveCharacterTextSplitter`, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66c6d5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 63 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1656279",
   "metadata": {},
   "source": [
    "Storing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82b0d016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7622ffd1-0b33-4440-b9a6-706c5a9e94e8', '61e1d2e8-7448-43a9-9c3c-82fe177bbdd1', '9f1898b3-1961-49b1-ae88-0d8d51cf9f16']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf37ca6",
   "metadata": {},
   "source": [
    "Retrieval and Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77817f1e",
   "metadata": {},
   "source": [
    "## How RAG Applications Work\n",
    "\n",
    "RAG applications commonly work as follows:\n",
    "\n",
    "1. **Retrieve:** Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
    "2. **Generate:** A model produces an answer using a prompt that includes both the question with the retrieved data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa2797",
   "metadata": {},
   "source": [
    "\n",
    "RAG agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "114fc25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0719fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a blog post. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "agent = create_agent(model, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5579aa42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000001E982707890>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3aa08dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\indroneel\\miniconda3\\Lib\\site-packages\\langsmith\\client.py:297: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the standard method for Task Decomposition?\n",
      "\n",
      "Once you get the answer, look up common extensions of that method.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (fc_647cf5b2-0d95-4319-babc-3877708c6ea4)\n",
      " Call ID: fc_647cf5b2-0d95-4319-babc-3877708c6ea4\n",
      "  Args:\n",
      "    query: standard method for Task Decomposition common extensions\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'start_index': 2578, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into Problem PDDL, then (2) requests a classical planner to generate a PDDL plan based on an existing Domain PDDL, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'start_index': 1638, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to think step by step to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the models thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-bfb8-7120-a451-f184f55ce099; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-bfd6-73d1-a90f-d2027b18d762; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-bfe4-7620-b523-ab0ba61eccf1; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-c15b-7213-9cfc-364ed2e1e1f7; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-c15d-74d0-a2f3-1b6c5c0cd104; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-c1e2-7801-b3fb-4adfa55076e2; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-c1e5-76f1-b5ac-762adf9ef3fb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (fc_61e12ab5-f297-41e8-bb76-5b27269bea8c)\n",
      " Call ID: fc_61e12ab5-f297-41e8-bb76-5b27269bea8c\n",
      "  Args:\n",
      "    query: extensions of chain of thought task decomposition Tree of Thoughts Self-Consistency\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'start_index': 1638, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to think step by step to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the models thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 40518}\n",
      "Content: Or\n",
      "@article{weng2023agent,\n",
      "  title   = \"LLM-powered Autonomous Agents\",\n",
      "  author  = \"Weng, Lilian\",\n",
      "  journal = \"lilianweng.github.io\",\n",
      "  year    = \"2023\",\n",
      "  month   = \"Jun\",\n",
      "  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
      "}\n",
      "References#\n",
      "[1] Wei et al. Chain of thought prompting elicits reasoning in large language models. NeurIPS 2022\n",
      "[2] Yao et al. Tree of Thoughts: Dliberate Problem Solving with Large Language Models. arXiv preprint arXiv:2305.10601 (2023).\n",
      "[3] Liu et al. Chain of Hindsight Aligns Language Models with Feedback\n",
      " arXiv preprint arXiv:2302.02676 (2023).\n",
      "[4] Liu et al. LLM+P: Empowering Large Language Models with Optimal Planning Proficiency arXiv preprint arXiv:2304.11477 (2023).\n",
      "[5] Yao et al. ReAct: Synergizing reasoning and acting in language models. ICLR 2023.\n",
      "[6] Google Blog. Announcing ScaNN: Efficient Vector Similarity Search July 28, 2020.\n",
      "[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (fc_6b385bc2-de29-40f5-8a5b-4d3dc2676926)\n",
      " Call ID: fc_6b385bc2-de29-40f5-8a5b-4d3dc2676926\n",
      "  Args:\n",
      "    query: Self-Consistency chain of thought extension\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'start_index': 40518, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Or\n",
      "@article{weng2023agent,\n",
      "  title   = \"LLM-powered Autonomous Agents\",\n",
      "  author  = \"Weng, Lilian\",\n",
      "  journal = \"lilianweng.github.io\",\n",
      "  year    = \"2023\",\n",
      "  month   = \"Jun\",\n",
      "  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
      "}\n",
      "References#\n",
      "[1] Wei et al. Chain of thought prompting elicits reasoning in large language models. NeurIPS 2022\n",
      "[2] Yao et al. Tree of Thoughts: Dliberate Problem Solving with Large Language Models. arXiv preprint arXiv:2305.10601 (2023).\n",
      "[3] Liu et al. Chain of Hindsight Aligns Language Models with Feedback\n",
      " arXiv preprint arXiv:2302.02676 (2023).\n",
      "[4] Liu et al. LLM+P: Empowering Large Language Models with Optimal Planning Proficiency arXiv preprint arXiv:2304.11477 (2023).\n",
      "[5] Yao et al. ReAct: Synergizing reasoning and acting in language models. ICLR 2023.\n",
      "[6] Google Blog. Announcing ScaNN: Efficient Vector Similarity Search July 28, 2020.\n",
      "[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\n",
      "\n",
      "Source: {'start_index': 6069, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Chain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$, where $\\leq i \\leq j \\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-c1e5-76f1-b5ac-762adf9ef3fb; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-c1e2-7801-b3fb-4adfa55076e2; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-c39a-7fb3-922c-9fd94ac7360b; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-c39b-73e1-a5ee-c691b94575d3; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-c39b-73e1-a5ee-c691b94575d3; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-c39a-7fb3-922c-9fd94ac7360b; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-c3c1-7691-9d9b-9d53c21ed580; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-c3c4-7710-9284-f0a06179dcd7; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-c3c4-7710-9284-f0a06179dcd7; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-c3c1-7691-9d9b-9d53c21ed580; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-c54d-7d72-90cd-f3b47467dbac; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-c54e-7d80-aa43-296f28f1660b; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-c54e-7d80-aa43-296f28f1660b; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-c54d-7d72-90cd-f3b47467dbac; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-c573-7151-9677-a5a26572881d; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-c576-74e1-a485-4636b05e2c0a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Standard method for TaskDecomposition**\n",
      "\n",
      "The defacto standard way to break a complex problem into smaller, tractable pieces for a large language model is **ChainofThought (CoT) prompting**.  In CoT the model is explicitly instructed to think stepbystep, which forces it to generate a sequence of intermediate reasoning steps (or subgoals) before producing the final answer.  This simple prompting trick has become the baseline for most recent work on reasoningheavy taskssource1638.\n",
      "\n",
      "---\n",
      "\n",
      "**Common extensions of ChainofThought**\n",
      "\n",
      "| Extension | How it builds on CoT | Key idea / benefit |\n",
      "|-----------|----------------------|--------------------|\n",
      "| **Tree of Thoughts (ToT)** | Instead of a single linear chain, ToT generates *multiple* candidate thoughts at each reasoning step and organizes them into a tree.  A search algorithm (BFS or DFS) explores the tree, evaluating each node with a classifier or majority vote to pick the most promising pathsource1638. |\n",
      "| **Chain of Hindsight (CoH)** | After an initial CoT run, the model is shown a *sequence of its own past outputs together with humanprovided feedback* (ratings and hindsight comments).  It is then finetuned (or prompted) to produce a better final answer by reflecting on that feedbacksource6069. |\n",
      "| **SelfConsistency (SC)** *(not in the excerpt but widely cited)* | The model samples many independent CoT traces and returns the answer that appears most frequently (or has the highest aggregated confidence).  This reduces variance and often yields more reliable results. |\n",
      "| **LLM+Planner (LLM+P)** | The CoT idea is combined with an *external classical planner*.  The LLM translates the problem into PDDL, a planner solves it, and the plan is translated back to natural language.  This offloads the combinatorial search to a dedicated planner while still using CoTstyle decompositionsource2578. |\n",
      "\n",
      "These extensions share the same core principle**explicitly decompose a hard task into intermediate steps**but they add richer search, feedbackdriven refinement, or external planning to improve robustness and performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-c576-74e1-a485-4636b05e2c0a; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-c573-7151-9677-a5a26572881d; trace=019afe88-bfb8-7120-a451-f184f55ce099,id=019afe88-bfb8-7120-a451-f184f55ce099\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
    "    \"Once you get the answer, look up common extensions of that method.\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360d2862",
   "metadata": {},
   "source": [
    "\n",
    "RAG chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d1a91e",
   "metadata": {},
   "source": [
    "## Agentic RAG: Trade-offs and Approaches\n",
    "\n",
    "In the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n",
    "\n",
    "###  Benefits\n",
    "\n",
    "- **Search only when needed**  The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\n",
    "- **Contextual search queries**  By treating search as a tool with a `query` input, the LLM crafts its own queries that incorporate conversational context.\n",
    "- **Multiple searches allowed**  The LLM can execute several searches in support of a single user query.\n",
    "\n",
    "###  Drawbacks\n",
    "\n",
    "- **Two inference calls**  When a search is performed, it requires one call to generate the query and another to produce the final response.\n",
    "- **Reduced control**  The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\n",
    "\n",
    "---\n",
    "\n",
    "## Alternative Approach: Two-Step Chain\n",
    "\n",
    "Another common approach is a **two-step chain**, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\n",
    "\n",
    "In this approach we no longer call the model in a loop, but instead make a single pass.\n",
    "\n",
    "We can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "493a7584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    retrieved_docs = vector_store.similarity_search(last_query)\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant. Use the following context in your response:\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "\n",
    "    return system_message\n",
    "\n",
    "\n",
    "agent = create_agent(model, tools=[], middleware=[prompt_with_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03c7be4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000001E9825AF360>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a473549a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is task decomposition?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=019afeb1-be0d-7e30-bc9f-719012f3ec9d,id=019afeb1-be0d-7e30-bc9f-719012f3ec9d; trace=019afeb1-be0d-7e30-bc9f-719012f3ec9d,id=019afeb1-be15-77f3-a4c1-b06da6a4f517; trace=019afeb1-be0d-7e30-bc9f-719012f3ec9d,id=019afeb1-be4e-7103-b99c-7a68cb9fd2bb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Answer**\n",
      "\n",
      "Task decomposition is the process of breaking a complex, highlevel goal into a set of smaller, more manageable subtasks or steps. By dividing a big problem into simpler pieces, an agent (human or AI) can plan, reason, and execute each part more effectively, often improving overall performance and interpretability.\n",
      "\n",
      "---\n",
      "\n",
      "### How I arrived at this answer  \n",
      "\n",
      "**User Input:**  \n",
      "*What is task decomposition?*\n",
      "\n",
      "**Task Planning:**  \n",
      "I identified a single, straightforward informationretrieval task:\n",
      "\n",
      "| Task ID | Task Type          | Dependencies | Arguments                     |\n",
      "|--------|--------------------|--------------|------------------------------|\n",
      "| T1     | Explain concept    |             | task decomposition definition |\n",
      "\n",
      "**Model Selection:**  \n",
      "Because the request only needs a concise, factual explanation, I assigned the task to a **languagemodel expert** specialized in knowledge retrieval and summarization (e.g., a GPT4 style model).\n",
      "\n",
      "**Task Execution (Predictions):**  \n",
      "The model generated the definition above, emphasizing that task decomposition involves splitting a complex objective into smaller subgoals, which aids planning, reasoning, and execution.\n",
      "\n",
      "---\n",
      "\n",
      "### Summary of the process (firstperson view)\n",
      "\n",
      "1. **Parsing the request**  I recognized that the user wanted a clear definition, so I framed it as a single explanatory task.  \n",
      "2. **Planning the task**  I created a task record (T1) with no dependencies, specifying that the argument is the term task decomposition.  \n",
      "3. **Choosing the model**  I selected the languagemodel expert because the task is purely textual and does not require external tools or specialized planners.  \n",
      "4. **Executing the task**  I ran the model, which produced the concise definition you see above.\n",
      "\n",
      "If you need more detailsuch as examples of how task decomposition is used in planning, chainofthought prompting, or robotic PDDL pipelinesjust let me know!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=019afeb1-be0d-7e30-bc9f-719012f3ec9d,id=019afeb1-be4e-7103-b99c-7a68cb9fd2bb; trace=019afeb1-be0d-7e30-bc9f-719012f3ec9d,id=019afeb1-be15-77f3-a4c1-b06da6a4f517; trace=019afeb1-be0d-7e30-bc9f-719012f3ec9d,id=019afeb1-be0d-7e30-bc9f-719012f3ec9d\n"
     ]
    }
   ],
   "source": [
    "query = \"What is task decomposition?\"\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
